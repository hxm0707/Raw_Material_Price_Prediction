{"cells":[{"source":"## TO-DOs\n```\n[v] Import electrcity data, transform to daily price\n[v] Import TTF_GAS data, transform to daily price\n[v] Import price evaluatioin data, filtering with Alkalis_RM02_0001\n[v] Create 21-days earlier features\n[v] Combine features with target variables\n!!! # Some issues exist when join historical factor prices with target variables\n[] Data scaling\n[] check multicollinearity\n[] train_test_split()\n[] Lasso regression\n[] Cross validation\n```","metadata":{},"cell_type":"markdown","id":"6b4c7f02-1c48-419e-ae1c-979e205b1891"},{"source":"!pip install fredapi\n!pip install pandasql","metadata":{"executionCancelledAt":null,"executionTime":10631,"lastExecutedAt":1708382468268,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"!pip install fredapi\n!pip install pandasql","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"e79fdd88-9172-4097-9fa2-b91ab3eef2e5","outputs":[{"output_type":"stream","name":"stdout","text":"Defaulting to user installation because normal site-packages is not writeable\nCollecting fredapi\n  Downloading fredapi-0.5.1-py3-none-any.whl.metadata (5.0 kB)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from fredapi) (1.5.1)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (2022.7)\nRequirement already satisfied: numpy>=1.20.3 in /usr/local/lib/python3.8/dist-packages (from pandas->fredapi) (1.23.2)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->fredapi) (1.14.0)\nDownloading fredapi-0.5.1-py3-none-any.whl (11 kB)\nInstalling collected packages: fredapi\nSuccessfully installed fredapi-0.5.1\nDefaulting to user installation because normal site-packages is not writeable\nRequirement already satisfied: pandasql in /usr/local/lib/python3.8/dist-packages (0.7.3)\nRequirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.23.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.5.1)\nRequirement already satisfied: sqlalchemy in /usr/local/lib/python3.8/dist-packages (from pandasql) (1.4.40)\nRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.8/dist-packages (from pandas->pandasql) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.8/dist-packages (from pandas->pandasql) (2022.7)\nRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.8/dist-packages (from sqlalchemy->pandasql) (1.1.3)\nRequirement already satisfied: six>=1.5 in /usr/lib/python3/dist-packages (from python-dateutil>=2.8.1->pandas->pandasql) (1.14.0)\n"}],"execution_count":1},{"source":"import pandas as pd\nfrom fredapi import Fred\nfrom pandasql import sqldf\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt","metadata":{"executionCancelledAt":null,"executionTime":3165,"lastExecutedAt":1708382471435,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"import pandas as pd\nfrom fredapi import Fred\nfrom pandasql import sqldf\nfrom sklearn.linear_model import Lasso\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nimport matplotlib.pyplot as plt"},"cell_type":"code","id":"d95bfcf0-14be-44fb-9a51-ceefcff17281","outputs":[],"execution_count":2},{"source":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","metadata":{"executionCancelledAt":null,"executionTime":54,"lastExecutedAt":1708382471490,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"def monthly_mean_to_daily(df_monthly: pd.core.frame.DataFrame ) -> pd.core.frame.DataFrame:\n    \"\"\"\n    Convert Monthly data into Daily data and impute with monthly mean prices\n    \"\"\"\n    df_monthly['Date'] = pd.to_datetime(df_monthly[['Year', 'Month']].assign(DAY=1))\n    df = df_monthly.explode('Date') # The explode() method converts each element of the specified column(s) into a row.\n\n    # Generate a complete range of daily dates for the year for imputation\n    start_date = df['Date'].min() # represents the starting point of your data\n    end_date = df['Date'].max() + pd.offsets.MonthEnd(1)  # finds the maximum (or latest) date and include the last month fully\n    full_date_range = pd.date_range(start=start_date, end=end_date, freq='D') # generates a fixed-frequency DatetimeIndex\n\n    # Merge the full date range with the monthly averages to fill in all days\n    df_full_date_range = pd.DataFrame(full_date_range, columns=['Date'])\n    df = pd.merge(df_full_date_range, df_monthly, on='Date', how='left')\n    df_daily = df.ffill(axis=0) # to fill the missing value based on last valid observation following index sequence\n    return df_daily","collapsed":false,"jupyter":{"outputs_hidden":false,"source_hidden":true}},"cell_type":"code","id":"a8ebea83-36e8-4355-b23f-c3b4a33b8602","outputs":[],"execution_count":3},{"source":"## To import Electricity price from 2012 to 2023, and extend daily prices with monthly mean prices.","metadata":{},"cell_type":"markdown","id":"5814643c-8edf-4cfe-aa1b-b0977a7c84ca"},{"source":"elec_df_monthly = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df_daily = monthly_mean_to_daily(elec_df_monthly)\n\nelec_df_daily = elec_df_daily[elec_df_daily['Year']>=2012].reset_index().drop(['index'], axis=1)\n\nprint(elec_df_daily)\nprint(elec_df_daily.isna().sum().sort_values()) # checking missing values","metadata":{"executionCancelledAt":null,"executionTime":52,"lastExecutedAt":1708382471543,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"elec_df_monthly = pd.read_csv('ELECTRICITY.csv').iloc[:,1:]\nelec_df_daily = monthly_mean_to_daily(elec_df_monthly)\n\nelec_df_daily = elec_df_daily[elec_df_daily['Year']>=2012].reset_index().drop(['index'], axis=1)\n\nprint(elec_df_daily)\nprint(elec_df_daily.isna().sum().sort_values()) # checking missing values","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"0d2b23a4-a5aa-4657-9dbe-30997e9c3d42","outputs":[{"output_type":"stream","name":"stdout","text":"           Date    Year  Month  Electricity\n0    2012-01-01  2012.0    1.0        56.13\n1    2012-01-02  2012.0    1.0        56.13\n2    2012-01-03  2012.0    1.0        56.13\n3    2012-01-04  2012.0    1.0        56.13\n4    2012-01-05  2012.0    1.0        56.13\n...         ...     ...    ...          ...\n4378 2023-12-27  2023.0   12.0       112.54\n4379 2023-12-28  2023.0   12.0       112.54\n4380 2023-12-29  2023.0   12.0       112.54\n4381 2023-12-30  2023.0   12.0       112.54\n4382 2023-12-31  2023.0   12.0       112.54\n\n[4383 rows x 4 columns]\nDate           0\nYear           0\nMonth          0\nElectricity    0\ndtype: int64\n"}],"execution_count":4},{"source":"## To import EU Gas price from 2012 to 2023, and extend daily prices with monthly mean prices.","metadata":{},"cell_type":"markdown","id":"98e0db76-7eff-48b0-94c8-3a23c0e1ffd2"},{"source":"apiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Natural Gas prices in Europe per month\nTTF_GAS = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_GAS['index'] = pd.to_datetime(TTF_GAS['index'], format='%Y-%m-%d')\nTTF_GAS['Year'] = TTF_GAS['index'].dt.year\nTTF_GAS['Month'] = TTF_GAS['index'].dt.month\nTTF_GAS = TTF_GAS.drop(['index'], axis=1)\nTTF_GAS_2012_23_monthly = TTF_GAS[TTF_GAS['Year']>=2012].reset_index().drop(['index'], axis=1)\n\nTTF_GAS_daily = monthly_mean_to_daily(TTF_GAS_2012_23_monthly)\nprint(TTF_GAS_daily.info())\nprint(TTF_GAS_daily.isna().sum().sort_values()) # Check missing values\n\n\n","metadata":{"executionCancelledAt":null,"executionTime":324,"lastExecutedAt":1708382471867,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"apiKey = '29219060bc68b2802af8584e0f328b52'\nfred = Fred(api_key=apiKey)\n\n# Natural Gas prices in Europe per month\nTTF_GAS = pd.DataFrame(fred.get_series('PNGASEUUSDM'), \n                       columns=['PNGASEUUSDM']).reset_index() \nTTF_GAS['index'] = pd.to_datetime(TTF_GAS['index'], format='%Y-%m-%d')\nTTF_GAS['Year'] = TTF_GAS['index'].dt.year\nTTF_GAS['Month'] = TTF_GAS['index'].dt.month\nTTF_GAS = TTF_GAS.drop(['index'], axis=1)\nTTF_GAS_2012_23_monthly = TTF_GAS[TTF_GAS['Year']>=2012].reset_index().drop(['index'], axis=1)\n\nTTF_GAS_daily = monthly_mean_to_daily(TTF_GAS_2012_23_monthly)\nprint(TTF_GAS_daily.info())\nprint(TTF_GAS_daily.isna().sum().sort_values()) # Check missing values\n\n\n","outputsMetadata":{"0":{"height":357,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"b71f7113-0cfa-4a46-8130-9b07e18d20a9","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 4383 entries, 0 to 4382\nData columns (total 4 columns):\n #   Column       Non-Null Count  Dtype         \n---  ------       --------------  -----         \n 0   Date         4383 non-null   datetime64[ns]\n 1   PNGASEUUSDM  4383 non-null   float64       \n 2   Year         4383 non-null   float64       \n 3   Month        4383 non-null   float64       \ndtypes: datetime64[ns](1), float64(3)\nmemory usage: 171.2 KB\nNone\nDate           0\nPNGASEUUSDM    0\nYear           0\nMonth          0\ndtype: int64\n"}],"execution_count":5},{"source":"price_evolutions_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:].sort_values(by=['POSTING DATE', 'Key RM code'])\nprice_evolutions_df = price_evolutions_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\nprice_evolutions_df['POSTING DATE'] = pd.to_datetime(price_evolutions_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evolutions_df['Year'] = price_evolutions_df['POSTING DATE'].dt.year\nprice_evolutions_df['Month'] = price_evolutions_df['POSTING DATE'].dt.month\nprice_evolutions_df.rename(columns={'POSTING DATE':'Date',\n                                   'Group Description':'Group_Description',\n                                   'Key RM code':'Key_RM_code',\n                                   'PRICE (EUR/kg)':'PRICE'},\n                                    inplace=True)\n\nprint(price_evolutions_df.info())\nprint(price_evolutions_df)\n","metadata":{"executionCancelledAt":null,"executionTime":103,"lastExecutedAt":1708382471970,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"price_evolutions_df = pd.read_csv('Dataset_Predicting_Price_Evolutions.csv').iloc[:,1:].sort_values(by=['POSTING DATE', 'Key RM code'])\nprice_evolutions_df = price_evolutions_df.drop(['SITE', 'SUPPLIER NUMBER', 'PURCHASE NUMBER', 'WEIGHT (kg)'], axis=1)\nprice_evolutions_df['POSTING DATE'] = pd.to_datetime(price_evolutions_df['POSTING DATE'], format='%Y-%m-%d')\nprice_evolutions_df['Year'] = price_evolutions_df['POSTING DATE'].dt.year\nprice_evolutions_df['Month'] = price_evolutions_df['POSTING DATE'].dt.month\nprice_evolutions_df.rename(columns={'POSTING DATE':'Date',\n                                   'Group Description':'Group_Description',\n                                   'Key RM code':'Key_RM_code',\n                                   'PRICE (EUR/kg)':'PRICE'},\n                                    inplace=True)\n\nprint(price_evolutions_df.info())\nprint(price_evolutions_df)\n","outputsMetadata":{"0":{"height":377,"type":"stream"}},"collapsed":true,"jupyter":{"outputs_hidden":true,"source_hidden":false}},"cell_type":"code","id":"af06d94f-1218-4363-a83d-f5ac1783022a","outputs":[{"output_type":"stream","name":"stdout","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 20570 entries, 20534 to 16\nData columns (total 6 columns):\n #   Column             Non-Null Count  Dtype         \n---  ------             --------------  -----         \n 0   Date               20570 non-null  datetime64[ns]\n 1   Group_Description  20570 non-null  object        \n 2   Key_RM_code        20570 non-null  object        \n 3   PRICE              20570 non-null  float64       \n 4   Year               20570 non-null  int64         \n 5   Month              20570 non-null  int64         \ndtypes: datetime64[ns](1), float64(1), int64(2), object(2)\nmemory usage: 1.1+ MB\nNone\n            Date     Group_Description Key_RM_code     PRICE  Year  Month\n20534 2012-01-31                  acid   RM01/0001  0.500000  2012      1\n20547 2012-01-31                  acid   RM01/0001  0.492479  2012      1\n20560 2012-01-31                  acid   RM01/0001  0.485000  2012      1\n20569 2012-01-31                  acid   RM01/0001  0.500000  2012      1\n20538 2012-01-31                  acid   RM01/0004  0.650000  2012      1\n...          ...                   ...         ...       ...   ...    ...\n12    2023-10-31  non-ionic surfactant   RM12/0012  2.500000  2023     10\n0     2023-10-31            fatty acid   RM14/0001  1.298300  2023     10\n9     2023-10-31               builder   RM20/0001  1.480000  2023     10\n2     2023-10-31               builder   RM20/0020  0.790000  2023     10\n16    2023-10-31               builder   RM20/0020  0.965311  2023     10\n\n[20570 rows x 6 columns]\n"}],"execution_count":6},{"source":"df = price_evolutions_df[price_evolutions_df['Group_Description']=='Alkalis']\n\n# Function to use pandasql\n# pysqldf = lambda q: sqldf(q, globals())\n# query = \"\"\"\n# SELECT Key_RM_code, count(Key_RM_code) \n# FROM df\n# GROUP BY Key_RM_code;\n# \"\"\"\n# print(pysqldf(query))\n#   Key_RM_code  count(Key_RM_code)\n# 0   RM02/0001                5293\n# 1   RM02/0002                 714\n\n# print(df.info())\n\nlabeled_date = df['Date'] - pd.Timedelta(days=21)\n# print(date_labels.head(10))\n\ndf_0 = pd.merge(TTF_GAS_daily, elec_df_daily,how='left', on = ['Date', 'Year', 'Month'])\n# print(df_0.head(10))\ntwenty_one_days_earlier_feature_df = df_0[df_0['Date'].isin(labeled_date)]\n\ntwenty_one_days_earlier_feature_df.to_csv('twenty_one_days_earlier_feature_df.csv',index=False)\n# twenty_one_days_earlier_feature_df.rename(columns={'Date':'History_Date'},inplace=True)\n# df['History_Date']=df['Date'] - pd.Timedelta(days=21)\n# df = pd.merge(df, twenty_one_days_earlier_feature_df, how='left', on = ['History_Date','Year', 'Month'])\n# df = df.drop(['History_Date'], axis=1)\n\n# Alkalis_RM02_0001 = df[df['Key_RM_code'] == 'RM02/0001']\n# Alkalis_RM02_0001.rename(columns={'Electricity':'History_Electricity',\n#                                    'PNGASEUUSDM':'History_PNGASEUUSDM'\n#                                  },\n#                                     inplace=True)\n# print(Alkalis_RM02_0001.isna().sum().sort_values()) # Some issues exist when join historical factor prices with target variables\n","metadata":{"executionCancelledAt":null,"executionTime":56,"lastExecutedAt":1708382941071,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"df = price_evolutions_df[price_evolutions_df['Group_Description']=='Alkalis']\n\n# Function to use pandasql\n# pysqldf = lambda q: sqldf(q, globals())\n# query = \"\"\"\n# SELECT Key_RM_code, count(Key_RM_code) \n# FROM df\n# GROUP BY Key_RM_code;\n# \"\"\"\n# print(pysqldf(query))\n#   Key_RM_code  count(Key_RM_code)\n# 0   RM02/0001                5293\n# 1   RM02/0002                 714\n\n# print(df.info())\n\nlabeled_date = df['Date'] - pd.Timedelta(days=21)\n# print(date_labels.head(10))\n\ndf_0 = pd.merge(TTF_GAS_daily, elec_df_daily,how='left', on = ['Date', 'Year', 'Month'])\n# print(df_0.head(10))\ntwenty_one_days_earlier_feature_df = df_0[df_0['Date'].isin(labeled_date)]\n\ntwenty_one_days_earlier_feature_df.to_csv('twenty_one_days_earlier_feature_df.csv',index=False)\n# twenty_one_days_earlier_feature_df.rename(columns={'Date':'History_Date'},inplace=True)\n# df['History_Date']=df['Date'] - pd.Timedelta(days=21)\n# df = pd.merge(df, twenty_one_days_earlier_feature_df, how='left', on = ['History_Date','Year', 'Month'])\n# df = df.drop(['History_Date'], axis=1)\n\n# Alkalis_RM02_0001 = df[df['Key_RM_code'] == 'RM02/0001']\n# Alkalis_RM02_0001.rename(columns={'Electricity':'History_Electricity',\n#                                    'PNGASEUUSDM':'History_PNGASEUUSDM'\n#                                  },\n#                                     inplace=True)\n# print(Alkalis_RM02_0001.isna().sum().sort_values()) # Some issues exist when join historical factor prices with target variables\n","outputsMetadata":{"0":{"height":77,"type":"stream"}}},"cell_type":"code","id":"e17a4b30-0f93-4c32-adad-cc405c22b1bd","outputs":[],"execution_count":13},{"source":"# Data scaling\n# check multicollinearity\n# train_test_split()\n\ny = Alkalis_RM02_0001['PRICE'].values\n\n\n# Looping through features\nfor feature in [\"History_PNGASEUUSDM\", \"History_Electricity\"]:\n    X = df[[feature]].values\n\n    # Split the data into training and test sets, setting test_size equal to 30% and using a random_state of 42.\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, \n        y, \n        test_size=0.3, \n        random_state=42)\n    print(np.isnan(X_train))\n    print(np.isnan(X_test))\n    print(np.isnan(y_train))\n    print(np.isnan(y_test))\n\n#     # Feature scaling\n#     scaler = StandardScaler() \n#     X_train_scaled = scaler.fit_transform(X_train) \n#     X_test_scaled = scaler.transform(X_test)\n    \n\n#     scores = []\n#     for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:\n#         lasso = Lasso(alpha=alpha)\n#         lasso.fit(X_train_scaled, y_train)\n#         lasso_pred = lasso.predict(X_test_scaled)\n#         scores.append(lasso.score(X_test_scaled, y_test))\n#         print(scores)\n\n\n# # 4. Estimating feature correlation\n# sns.heatmap(Alkalis_RM02_0001.drop(['Date','Group_Description','Key_RM_code','Year','Month','PRICE'], axis=1).corr(),annot=True)\n# plt.show()\n\n# # Select the final features for the model\n# final_features = [\"N\", \"K\", \"ph\"] # 'P', 'K' have strong correlations, but the f1 score of 'K' is higher than than the one of 'P'.\n\n# # 5. Producing a final model\n# X = crops[final_features].values\n\n# # Splitting the data with final_features\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X, \n#     y, \n#     test_size=0.2, \n#     random_state=42)\n\n# # Feature scaling\n# scaler = StandardScaler() \n# X_train_scaled = scaler.fit_transform(X_train) \n# X_test_scaled = scaler.transform(X_test)\n\n# # Train and evaluate the model\n# log_reg = LogisticRegression(max_iter=2000, multi_class=\"multinomial\")\n# log_reg.fit(X_train_scaled, y_train)\n# y_pred = log_reg.predict(X_test_scaled)\n# model_performance=f1_score(y_test,y_pred,average=\"weighted\")\n# print(model_performance)\n","metadata":{"executionCancelledAt":null,"executionTime":2722,"lastExecutedAt":1708382474744,"lastScheduledRunId":null,"lastSuccessfullyExecutedCode":"# Data scaling\n# check multicollinearity\n# train_test_split()\n\ny = Alkalis_RM02_0001['PRICE'].values\n\n\n# Looping through features\nfor feature in [\"History_PNGASEUUSDM\", \"History_Electricity\"]:\n    X = df[[feature]].values\n\n    # Split the data into training and test sets, setting test_size equal to 30% and using a random_state of 42.\n    X_train, X_test, y_train, y_test = train_test_split(\n        X, \n        y, \n        test_size=0.3, \n        random_state=42)\n    print(np.isnan(X_train))\n    print(np.isnan(X_test))\n    print(np.isnan(y_train))\n    print(np.isnan(y_test))\n\n#     # Feature scaling\n#     scaler = StandardScaler() \n#     X_train_scaled = scaler.fit_transform(X_train) \n#     X_test_scaled = scaler.transform(X_test)\n    \n\n#     scores = []\n#     for alpha in [0.01, 1.0, 10.0, 20.0, 50.0]:\n#         lasso = Lasso(alpha=alpha)\n#         lasso.fit(X_train_scaled, y_train)\n#         lasso_pred = lasso.predict(X_test_scaled)\n#         scores.append(lasso.score(X_test_scaled, y_test))\n#         print(scores)\n\n\n# # 4. Estimating feature correlation\n# sns.heatmap(Alkalis_RM02_0001.drop(['Date','Group_Description','Key_RM_code','Year','Month','PRICE'], axis=1).corr(),annot=True)\n# plt.show()\n\n# # Select the final features for the model\n# final_features = [\"N\", \"K\", \"ph\"] # 'P', 'K' have strong correlations, but the f1 score of 'K' is higher than than the one of 'P'.\n\n# # 5. Producing a final model\n# X = crops[final_features].values\n\n# # Splitting the data with final_features\n# X_train, X_test, y_train, y_test = train_test_split(\n#     X, \n#     y, \n#     test_size=0.2, \n#     random_state=42)\n\n# # Feature scaling\n# scaler = StandardScaler() \n# X_train_scaled = scaler.fit_transform(X_train) \n# X_test_scaled = scaler.transform(X_test)\n\n# # Train and evaluate the model\n# log_reg = LogisticRegression(max_iter=2000, multi_class=\"multinomial\")\n# log_reg.fit(X_train_scaled, y_train)\n# y_pred = log_reg.predict(X_test_scaled)\n# model_performance=f1_score(y_test,y_pred,average=\"weighted\")\n# print(model_performance)\n","outputsMetadata":{"0":{"height":377,"type":"stream"}}},"cell_type":"code","id":"d993db3b-d937-48ee-81cb-ec566a56ef14","outputs":[{"output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","Cell \u001b[0;32mIn[8], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Looping through features\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistory_PNGASEUUSDM\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHistory_Electricity\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[0;32m---> 10\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# Split the data into training and test sets, setting test_size equal to 30% and using a random_state of 42.\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     X_train, X_test, y_train, y_test \u001b[38;5;241m=\u001b[39m train_test_split(\n\u001b[1;32m     14\u001b[0m         X, \n\u001b[1;32m     15\u001b[0m         y, \n\u001b[1;32m     16\u001b[0m         test_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.3\u001b[39m, \n\u001b[1;32m     17\u001b[0m         random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/frame.py:3810\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3808\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   3809\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 3810\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   3812\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:6111\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6108\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6109\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6111\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6113\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6115\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n","File \u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/core/indexes/base.py:6171\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_interval_msg:\n\u001b[1;32m   6170\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 6171\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6173\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[1;32m   6174\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n","\u001b[0;31mKeyError\u001b[0m: \"None of [Index(['History_PNGASEUUSDM'], dtype='object')] are in the [columns]\""],"ename":"KeyError","evalue":"\"None of [Index(['History_PNGASEUUSDM'], dtype='object')] are in the [columns]\""}],"execution_count":20}],"metadata":{"language_info":{"name":"python","version":"3.8.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"editor":"DataCamp Workspace"},"nbformat":4,"nbformat_minor":5}