{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvaifgq3GWuJUGeBDZps6j",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hxm0707/Raw_Material_Price_Prediction/blob/main/Acid(LASSO).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Hrr6v3TOFN5X"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.linear_model import Lasso\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, RandomizedSearchCV\n",
        "from sklearn.metrics import mean_absolute_percentage_error\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-RwR4q52VHCS",
        "outputId": "e097b9b8-0062-44c3-8eff-c85f25c1da6d"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/Thesis/dataframe/acid_inter_terms.csv',index_col=0)\n",
        "feature_df['Time'] = pd.to_datetime(feature_df['Time'])\n",
        "print(feature_df)"
      ],
      "metadata": {
        "id": "haTO1095FgC8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fef00067-c5af-4953-eb6f-0c377e40f49a"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "     RM01/0001  RM01/0004  RM01/0006  RM01/0007  Year  Month       Time  \\\n",
            "2        False       True      False      False  2016      1 2016-01-31   \n",
            "3         True      False      False      False  2016      1 2016-01-31   \n",
            "4        False       True      False      False  2016      2 2016-02-29   \n",
            "5         True      False      False      False  2016      2 2016-02-29   \n",
            "6        False       True      False      False  2016      3 2016-03-31   \n",
            "..         ...        ...        ...        ...   ...    ...        ...   \n",
            "400      False      False       True      False  2023      8 2023-08-31   \n",
            "401      False      False       True      False  2023      9 2023-09-28   \n",
            "402      False      False       True      False  2023     10 2023-10-30   \n",
            "403      False      False       True      False  2023     11 2023-11-30   \n",
            "404      False      False       True      False  2023     12 2023-12-22   \n",
            "\n",
            "    Group Description  Average_price  PNGASEUUSDM_1  ...      AR_3      AR_4  \\\n",
            "2                acid       0.404800       5.810000  ...  0.359400  0.475714   \n",
            "3                acid       0.528015       5.810000  ...  0.549394  0.558730   \n",
            "4                acid       0.404800       5.090000  ...  0.358000  0.359400   \n",
            "5                acid       0.532218       5.090000  ...  0.550917  0.549394   \n",
            "6                acid       0.356000       4.790000  ...  0.358000  0.358000   \n",
            "..                ...            ...            ...  ...       ...       ...   \n",
            "400              acid       1.258544       9.611719  ...  1.669344  1.874301   \n",
            "401              acid       1.432129      10.717421  ...  1.641858  1.669344   \n",
            "402              acid       1.327096      11.427930  ...  1.456344  1.641858   \n",
            "403              acid       1.080000      13.388843  ...  1.258544  1.456344   \n",
            "404              acid       1.447941      13.714316  ...  1.432129  1.258544   \n",
            "\n",
            "         AR_5      AR_6      AR_7      AR_8      AR_9     AR_10     AR_11  \\\n",
            "2    0.426000  0.462500  0.451000  0.486000  0.446333  0.477200  0.445000   \n",
            "3    0.559098  0.558889  0.546843  0.550665  0.546185  0.556000  0.558667   \n",
            "4    0.475714  0.426000  0.462500  0.451000  0.486000  0.446333  0.477200   \n",
            "5    0.558730  0.559098  0.558889  0.546843  0.550665  0.546185  0.556000   \n",
            "6    0.359400  0.475714  0.426000  0.462500  0.451000  0.486000  0.446333   \n",
            "..        ...       ...       ...       ...       ...       ...       ...   \n",
            "400  2.073844  2.767510  2.161415  2.386413  2.550000  2.682106  2.588566   \n",
            "401  1.874301  2.073844  2.767510  2.161415  2.386413  2.550000  2.682106   \n",
            "402  1.669344  1.874301  2.073844  2.767510  2.161415  2.386413  2.550000   \n",
            "403  1.641858  1.669344  1.874301  2.073844  2.767510  2.161415  2.386413   \n",
            "404  1.456344  1.641858  1.669344  1.874301  2.073844  2.767510  2.161415   \n",
            "\n",
            "        AR_12  \n",
            "2    0.475333  \n",
            "3    0.560278  \n",
            "4    0.445000  \n",
            "5    0.558667  \n",
            "6    0.477200  \n",
            "..        ...  \n",
            "400  2.544955  \n",
            "401  2.588566  \n",
            "402  2.682106  \n",
            "403  2.550000  \n",
            "404  2.386413  \n",
            "\n",
            "[362 rows x 69 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_0001 = feature_df[feature_df['RM01/0001'] == 1]\n",
        "print(df_0001.info())"
      ],
      "metadata": {
        "id": "yxUjcQdLKJ_U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d1166490-635c-46f6-a971-6f5f2aebd6fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 96 entries, 3 to 193\n",
            "Data columns (total 69 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   RM01/0001          96 non-null     bool          \n",
            " 1   RM01/0004          96 non-null     bool          \n",
            " 2   RM01/0006          96 non-null     bool          \n",
            " 3   RM01/0007          96 non-null     bool          \n",
            " 4   Year               96 non-null     int64         \n",
            " 5   Month              96 non-null     int64         \n",
            " 6   Time               96 non-null     datetime64[ns]\n",
            " 7   Group Description  96 non-null     object        \n",
            " 8   Average_price      96 non-null     float64       \n",
            " 9   PNGASEUUSDM_1      96 non-null     float64       \n",
            " 10  PWHEAMTUSDM_1      96 non-null     float64       \n",
            " 11  WPU0652013A_1      96 non-null     float64       \n",
            " 12  Electricity_1      96 non-null     float64       \n",
            " 13  PNGASEUUSDM_2      96 non-null     float64       \n",
            " 14  PWHEAMTUSDM_2      96 non-null     float64       \n",
            " 15  WPU0652013A_2      96 non-null     float64       \n",
            " 16  Electricity_2      96 non-null     float64       \n",
            " 17  PNGASEUUSDM_3      96 non-null     float64       \n",
            " 18  PWHEAMTUSDM_3      96 non-null     float64       \n",
            " 19  WPU0652013A_3      96 non-null     float64       \n",
            " 20  Electricity_3      96 non-null     float64       \n",
            " 21  PNGASEUUSDM_4      96 non-null     float64       \n",
            " 22  PWHEAMTUSDM_4      96 non-null     float64       \n",
            " 23  WPU0652013A_4      96 non-null     float64       \n",
            " 24  Electricity_4      96 non-null     float64       \n",
            " 25  PNGASEUUSDM_5      96 non-null     float64       \n",
            " 26  PWHEAMTUSDM_5      96 non-null     float64       \n",
            " 27  WPU0652013A_5      96 non-null     float64       \n",
            " 28  Electricity_5      96 non-null     float64       \n",
            " 29  PNGASEUUSDM_6      96 non-null     float64       \n",
            " 30  PWHEAMTUSDM_6      96 non-null     float64       \n",
            " 31  WPU0652013A_6      96 non-null     float64       \n",
            " 32  Electricity_6      96 non-null     float64       \n",
            " 33  PNGASEUUSDM_7      96 non-null     float64       \n",
            " 34  PWHEAMTUSDM_7      96 non-null     float64       \n",
            " 35  WPU0652013A_7      96 non-null     float64       \n",
            " 36  Electricity_7      96 non-null     float64       \n",
            " 37  PNGASEUUSDM_8      96 non-null     float64       \n",
            " 38  PWHEAMTUSDM_8      96 non-null     float64       \n",
            " 39  WPU0652013A_8      96 non-null     float64       \n",
            " 40  Electricity_8      96 non-null     float64       \n",
            " 41  PNGASEUUSDM_9      96 non-null     float64       \n",
            " 42  PWHEAMTUSDM_9      96 non-null     float64       \n",
            " 43  WPU0652013A_9      96 non-null     float64       \n",
            " 44  Electricity_9      96 non-null     float64       \n",
            " 45  PNGASEUUSDM_10     96 non-null     float64       \n",
            " 46  PWHEAMTUSDM_10     96 non-null     float64       \n",
            " 47  WPU0652013A_10     96 non-null     float64       \n",
            " 48  Electricity_10     96 non-null     float64       \n",
            " 49  PNGASEUUSDM_11     96 non-null     float64       \n",
            " 50  PWHEAMTUSDM_11     96 non-null     float64       \n",
            " 51  WPU0652013A_11     96 non-null     float64       \n",
            " 52  Electricity_11     96 non-null     float64       \n",
            " 53  PNGASEUUSDM_12     96 non-null     float64       \n",
            " 54  PWHEAMTUSDM_12     96 non-null     float64       \n",
            " 55  WPU0652013A_12     96 non-null     float64       \n",
            " 56  Electricity_12     96 non-null     float64       \n",
            " 57  AR_1               96 non-null     float64       \n",
            " 58  AR_2               96 non-null     float64       \n",
            " 59  AR_3               96 non-null     float64       \n",
            " 60  AR_4               96 non-null     float64       \n",
            " 61  AR_5               96 non-null     float64       \n",
            " 62  AR_6               96 non-null     float64       \n",
            " 63  AR_7               96 non-null     float64       \n",
            " 64  AR_8               96 non-null     float64       \n",
            " 65  AR_9               96 non-null     float64       \n",
            " 66  AR_10              96 non-null     float64       \n",
            " 67  AR_11              96 non-null     float64       \n",
            " 68  AR_12              96 non-null     float64       \n",
            "dtypes: bool(4), datetime64[ns](1), float64(61), int64(2), object(1)\n",
            "memory usage: 49.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_0004 = feature_df[feature_df['RM01/0004'] == 1]\n",
        "print(df_0004.info())"
      ],
      "metadata": {
        "id": "AW2knezKMSQo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51802dcb-c399-4471-e0b0-86d717ebbd22"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 96 entries, 2 to 192\n",
            "Data columns (total 69 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   RM01/0001          96 non-null     bool          \n",
            " 1   RM01/0004          96 non-null     bool          \n",
            " 2   RM01/0006          96 non-null     bool          \n",
            " 3   RM01/0007          96 non-null     bool          \n",
            " 4   Year               96 non-null     int64         \n",
            " 5   Month              96 non-null     int64         \n",
            " 6   Time               96 non-null     datetime64[ns]\n",
            " 7   Group Description  96 non-null     object        \n",
            " 8   Average_price      96 non-null     float64       \n",
            " 9   PNGASEUUSDM_1      96 non-null     float64       \n",
            " 10  PWHEAMTUSDM_1      96 non-null     float64       \n",
            " 11  WPU0652013A_1      96 non-null     float64       \n",
            " 12  Electricity_1      96 non-null     float64       \n",
            " 13  PNGASEUUSDM_2      96 non-null     float64       \n",
            " 14  PWHEAMTUSDM_2      96 non-null     float64       \n",
            " 15  WPU0652013A_2      96 non-null     float64       \n",
            " 16  Electricity_2      96 non-null     float64       \n",
            " 17  PNGASEUUSDM_3      96 non-null     float64       \n",
            " 18  PWHEAMTUSDM_3      96 non-null     float64       \n",
            " 19  WPU0652013A_3      96 non-null     float64       \n",
            " 20  Electricity_3      96 non-null     float64       \n",
            " 21  PNGASEUUSDM_4      96 non-null     float64       \n",
            " 22  PWHEAMTUSDM_4      96 non-null     float64       \n",
            " 23  WPU0652013A_4      96 non-null     float64       \n",
            " 24  Electricity_4      96 non-null     float64       \n",
            " 25  PNGASEUUSDM_5      96 non-null     float64       \n",
            " 26  PWHEAMTUSDM_5      96 non-null     float64       \n",
            " 27  WPU0652013A_5      96 non-null     float64       \n",
            " 28  Electricity_5      96 non-null     float64       \n",
            " 29  PNGASEUUSDM_6      96 non-null     float64       \n",
            " 30  PWHEAMTUSDM_6      96 non-null     float64       \n",
            " 31  WPU0652013A_6      96 non-null     float64       \n",
            " 32  Electricity_6      96 non-null     float64       \n",
            " 33  PNGASEUUSDM_7      96 non-null     float64       \n",
            " 34  PWHEAMTUSDM_7      96 non-null     float64       \n",
            " 35  WPU0652013A_7      96 non-null     float64       \n",
            " 36  Electricity_7      96 non-null     float64       \n",
            " 37  PNGASEUUSDM_8      96 non-null     float64       \n",
            " 38  PWHEAMTUSDM_8      96 non-null     float64       \n",
            " 39  WPU0652013A_8      96 non-null     float64       \n",
            " 40  Electricity_8      96 non-null     float64       \n",
            " 41  PNGASEUUSDM_9      96 non-null     float64       \n",
            " 42  PWHEAMTUSDM_9      96 non-null     float64       \n",
            " 43  WPU0652013A_9      96 non-null     float64       \n",
            " 44  Electricity_9      96 non-null     float64       \n",
            " 45  PNGASEUUSDM_10     96 non-null     float64       \n",
            " 46  PWHEAMTUSDM_10     96 non-null     float64       \n",
            " 47  WPU0652013A_10     96 non-null     float64       \n",
            " 48  Electricity_10     96 non-null     float64       \n",
            " 49  PNGASEUUSDM_11     96 non-null     float64       \n",
            " 50  PWHEAMTUSDM_11     96 non-null     float64       \n",
            " 51  WPU0652013A_11     96 non-null     float64       \n",
            " 52  Electricity_11     96 non-null     float64       \n",
            " 53  PNGASEUUSDM_12     96 non-null     float64       \n",
            " 54  PWHEAMTUSDM_12     96 non-null     float64       \n",
            " 55  WPU0652013A_12     96 non-null     float64       \n",
            " 56  Electricity_12     96 non-null     float64       \n",
            " 57  AR_1               96 non-null     float64       \n",
            " 58  AR_2               96 non-null     float64       \n",
            " 59  AR_3               96 non-null     float64       \n",
            " 60  AR_4               96 non-null     float64       \n",
            " 61  AR_5               96 non-null     float64       \n",
            " 62  AR_6               96 non-null     float64       \n",
            " 63  AR_7               96 non-null     float64       \n",
            " 64  AR_8               96 non-null     float64       \n",
            " 65  AR_9               96 non-null     float64       \n",
            " 66  AR_10              96 non-null     float64       \n",
            " 67  AR_11              96 non-null     float64       \n",
            " 68  AR_12              96 non-null     float64       \n",
            "dtypes: bool(4), datetime64[ns](1), float64(61), int64(2), object(1)\n",
            "memory usage: 49.9+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_0006 = feature_df[feature_df['RM01/0006'] == 1]\n",
        "print(df_0006.info())"
      ],
      "metadata": {
        "id": "9OYnvurmMTOY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c9324da9-159d-472c-e29a-76e656cf24b2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 95 entries, 309 to 404\n",
            "Data columns (total 69 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   RM01/0001          95 non-null     bool          \n",
            " 1   RM01/0004          95 non-null     bool          \n",
            " 2   RM01/0006          95 non-null     bool          \n",
            " 3   RM01/0007          95 non-null     bool          \n",
            " 4   Year               95 non-null     int64         \n",
            " 5   Month              95 non-null     int64         \n",
            " 6   Time               95 non-null     datetime64[ns]\n",
            " 7   Group Description  95 non-null     object        \n",
            " 8   Average_price      95 non-null     float64       \n",
            " 9   PNGASEUUSDM_1      95 non-null     float64       \n",
            " 10  PWHEAMTUSDM_1      95 non-null     float64       \n",
            " 11  WPU0652013A_1      95 non-null     float64       \n",
            " 12  Electricity_1      95 non-null     float64       \n",
            " 13  PNGASEUUSDM_2      95 non-null     float64       \n",
            " 14  PWHEAMTUSDM_2      95 non-null     float64       \n",
            " 15  WPU0652013A_2      95 non-null     float64       \n",
            " 16  Electricity_2      95 non-null     float64       \n",
            " 17  PNGASEUUSDM_3      95 non-null     float64       \n",
            " 18  PWHEAMTUSDM_3      95 non-null     float64       \n",
            " 19  WPU0652013A_3      95 non-null     float64       \n",
            " 20  Electricity_3      95 non-null     float64       \n",
            " 21  PNGASEUUSDM_4      95 non-null     float64       \n",
            " 22  PWHEAMTUSDM_4      95 non-null     float64       \n",
            " 23  WPU0652013A_4      95 non-null     float64       \n",
            " 24  Electricity_4      95 non-null     float64       \n",
            " 25  PNGASEUUSDM_5      95 non-null     float64       \n",
            " 26  PWHEAMTUSDM_5      95 non-null     float64       \n",
            " 27  WPU0652013A_5      95 non-null     float64       \n",
            " 28  Electricity_5      95 non-null     float64       \n",
            " 29  PNGASEUUSDM_6      95 non-null     float64       \n",
            " 30  PWHEAMTUSDM_6      95 non-null     float64       \n",
            " 31  WPU0652013A_6      95 non-null     float64       \n",
            " 32  Electricity_6      95 non-null     float64       \n",
            " 33  PNGASEUUSDM_7      95 non-null     float64       \n",
            " 34  PWHEAMTUSDM_7      95 non-null     float64       \n",
            " 35  WPU0652013A_7      95 non-null     float64       \n",
            " 36  Electricity_7      95 non-null     float64       \n",
            " 37  PNGASEUUSDM_8      95 non-null     float64       \n",
            " 38  PWHEAMTUSDM_8      95 non-null     float64       \n",
            " 39  WPU0652013A_8      95 non-null     float64       \n",
            " 40  Electricity_8      95 non-null     float64       \n",
            " 41  PNGASEUUSDM_9      95 non-null     float64       \n",
            " 42  PWHEAMTUSDM_9      95 non-null     float64       \n",
            " 43  WPU0652013A_9      95 non-null     float64       \n",
            " 44  Electricity_9      95 non-null     float64       \n",
            " 45  PNGASEUUSDM_10     95 non-null     float64       \n",
            " 46  PWHEAMTUSDM_10     95 non-null     float64       \n",
            " 47  WPU0652013A_10     95 non-null     float64       \n",
            " 48  Electricity_10     95 non-null     float64       \n",
            " 49  PNGASEUUSDM_11     95 non-null     float64       \n",
            " 50  PWHEAMTUSDM_11     95 non-null     float64       \n",
            " 51  WPU0652013A_11     95 non-null     float64       \n",
            " 52  Electricity_11     95 non-null     float64       \n",
            " 53  PNGASEUUSDM_12     95 non-null     float64       \n",
            " 54  PWHEAMTUSDM_12     95 non-null     float64       \n",
            " 55  WPU0652013A_12     95 non-null     float64       \n",
            " 56  Electricity_12     95 non-null     float64       \n",
            " 57  AR_1               95 non-null     float64       \n",
            " 58  AR_2               95 non-null     float64       \n",
            " 59  AR_3               95 non-null     float64       \n",
            " 60  AR_4               95 non-null     float64       \n",
            " 61  AR_5               95 non-null     float64       \n",
            " 62  AR_6               95 non-null     float64       \n",
            " 63  AR_7               95 non-null     float64       \n",
            " 64  AR_8               95 non-null     float64       \n",
            " 65  AR_9               95 non-null     float64       \n",
            " 66  AR_10              95 non-null     float64       \n",
            " 67  AR_11              95 non-null     float64       \n",
            " 68  AR_12              95 non-null     float64       \n",
            "dtypes: bool(4), datetime64[ns](1), float64(61), int64(2), object(1)\n",
            "memory usage: 49.4+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_0007 = feature_df[feature_df['RM01/0007'] == 1]\n",
        "print(df_0007.info())"
      ],
      "metadata": {
        "id": "BZ_zIuQlMVdl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "59a44af2-a553-4077-e8ce-6cb3c3f1ec1c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 75 entries, 213 to 307\n",
            "Data columns (total 69 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   RM01/0001          75 non-null     bool          \n",
            " 1   RM01/0004          75 non-null     bool          \n",
            " 2   RM01/0006          75 non-null     bool          \n",
            " 3   RM01/0007          75 non-null     bool          \n",
            " 4   Year               75 non-null     int64         \n",
            " 5   Month              75 non-null     int64         \n",
            " 6   Time               75 non-null     datetime64[ns]\n",
            " 7   Group Description  75 non-null     object        \n",
            " 8   Average_price      75 non-null     float64       \n",
            " 9   PNGASEUUSDM_1      75 non-null     float64       \n",
            " 10  PWHEAMTUSDM_1      75 non-null     float64       \n",
            " 11  WPU0652013A_1      75 non-null     float64       \n",
            " 12  Electricity_1      75 non-null     float64       \n",
            " 13  PNGASEUUSDM_2      75 non-null     float64       \n",
            " 14  PWHEAMTUSDM_2      75 non-null     float64       \n",
            " 15  WPU0652013A_2      75 non-null     float64       \n",
            " 16  Electricity_2      75 non-null     float64       \n",
            " 17  PNGASEUUSDM_3      75 non-null     float64       \n",
            " 18  PWHEAMTUSDM_3      75 non-null     float64       \n",
            " 19  WPU0652013A_3      75 non-null     float64       \n",
            " 20  Electricity_3      75 non-null     float64       \n",
            " 21  PNGASEUUSDM_4      75 non-null     float64       \n",
            " 22  PWHEAMTUSDM_4      75 non-null     float64       \n",
            " 23  WPU0652013A_4      75 non-null     float64       \n",
            " 24  Electricity_4      75 non-null     float64       \n",
            " 25  PNGASEUUSDM_5      75 non-null     float64       \n",
            " 26  PWHEAMTUSDM_5      75 non-null     float64       \n",
            " 27  WPU0652013A_5      75 non-null     float64       \n",
            " 28  Electricity_5      75 non-null     float64       \n",
            " 29  PNGASEUUSDM_6      75 non-null     float64       \n",
            " 30  PWHEAMTUSDM_6      75 non-null     float64       \n",
            " 31  WPU0652013A_6      75 non-null     float64       \n",
            " 32  Electricity_6      75 non-null     float64       \n",
            " 33  PNGASEUUSDM_7      75 non-null     float64       \n",
            " 34  PWHEAMTUSDM_7      75 non-null     float64       \n",
            " 35  WPU0652013A_7      75 non-null     float64       \n",
            " 36  Electricity_7      75 non-null     float64       \n",
            " 37  PNGASEUUSDM_8      75 non-null     float64       \n",
            " 38  PWHEAMTUSDM_8      75 non-null     float64       \n",
            " 39  WPU0652013A_8      75 non-null     float64       \n",
            " 40  Electricity_8      75 non-null     float64       \n",
            " 41  PNGASEUUSDM_9      75 non-null     float64       \n",
            " 42  PWHEAMTUSDM_9      75 non-null     float64       \n",
            " 43  WPU0652013A_9      75 non-null     float64       \n",
            " 44  Electricity_9      75 non-null     float64       \n",
            " 45  PNGASEUUSDM_10     75 non-null     float64       \n",
            " 46  PWHEAMTUSDM_10     75 non-null     float64       \n",
            " 47  WPU0652013A_10     75 non-null     float64       \n",
            " 48  Electricity_10     75 non-null     float64       \n",
            " 49  PNGASEUUSDM_11     75 non-null     float64       \n",
            " 50  PWHEAMTUSDM_11     75 non-null     float64       \n",
            " 51  WPU0652013A_11     75 non-null     float64       \n",
            " 52  Electricity_11     75 non-null     float64       \n",
            " 53  PNGASEUUSDM_12     75 non-null     float64       \n",
            " 54  PWHEAMTUSDM_12     75 non-null     float64       \n",
            " 55  WPU0652013A_12     75 non-null     float64       \n",
            " 56  Electricity_12     75 non-null     float64       \n",
            " 57  AR_1               75 non-null     float64       \n",
            " 58  AR_2               75 non-null     float64       \n",
            " 59  AR_3               75 non-null     float64       \n",
            " 60  AR_4               75 non-null     float64       \n",
            " 61  AR_5               75 non-null     float64       \n",
            " 62  AR_6               75 non-null     float64       \n",
            " 63  AR_7               75 non-null     float64       \n",
            " 64  AR_8               75 non-null     float64       \n",
            " 65  AR_9               75 non-null     float64       \n",
            " 66  AR_10              75 non-null     float64       \n",
            " 67  AR_11              75 non-null     float64       \n",
            " 68  AR_12              75 non-null     float64       \n",
            "dtypes: bool(4), datetime64[ns](1), float64(61), int64(2), object(1)\n",
            "memory usage: 39.0+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_periods = [\n",
        "    ('2019-01-01', '2019-07-01'),\n",
        "    ('2019-07-01', '2020-01-01'),\n",
        "    ('2020-01-01', '2020-07-01'),\n",
        "    ('2020-07-01', '2021-01-01'),\n",
        "    ('2021-01-01', '2021-07-01'),\n",
        "    ('2021-07-01', '2022-01-01'),\n",
        "    ('2022-01-01', '2022-07-01'),\n",
        "    ('2022-07-01', '2023-01-01'),\n",
        "    ('2023-01-01', '2023-07-01'),\n",
        "    ('2023-07-01', '2024-01-01'),\n",
        "]"
      ],
      "metadata": {
        "id": "OcLqU5vbL0Qn"
      },
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0001"
      ],
      "metadata": {
        "id": "VKpXeZyRKqvN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = df_0001"
      ],
      "metadata": {
        "id": "qDLPmHmLKtDo"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [column for column in feature_df.columns if 'RM' in column]\n",
        "feature_df = feature_df.drop(columns=columns_to_drop)\n",
        "print(feature_df.info())"
      ],
      "metadata": {
        "id": "HSVyHg_HKY_P",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a30c9ae2-2ce6-4d2f-dda9-5e7f3989145c"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Index: 96 entries, 3 to 193\n",
            "Data columns (total 65 columns):\n",
            " #   Column             Non-Null Count  Dtype         \n",
            "---  ------             --------------  -----         \n",
            " 0   Year               96 non-null     int64         \n",
            " 1   Month              96 non-null     int64         \n",
            " 2   Time               96 non-null     datetime64[ns]\n",
            " 3   Group Description  96 non-null     object        \n",
            " 4   Average_price      96 non-null     float64       \n",
            " 5   PNGASEUUSDM_1      96 non-null     float64       \n",
            " 6   PWHEAMTUSDM_1      96 non-null     float64       \n",
            " 7   WPU0652013A_1      96 non-null     float64       \n",
            " 8   Electricity_1      96 non-null     float64       \n",
            " 9   PNGASEUUSDM_2      96 non-null     float64       \n",
            " 10  PWHEAMTUSDM_2      96 non-null     float64       \n",
            " 11  WPU0652013A_2      96 non-null     float64       \n",
            " 12  Electricity_2      96 non-null     float64       \n",
            " 13  PNGASEUUSDM_3      96 non-null     float64       \n",
            " 14  PWHEAMTUSDM_3      96 non-null     float64       \n",
            " 15  WPU0652013A_3      96 non-null     float64       \n",
            " 16  Electricity_3      96 non-null     float64       \n",
            " 17  PNGASEUUSDM_4      96 non-null     float64       \n",
            " 18  PWHEAMTUSDM_4      96 non-null     float64       \n",
            " 19  WPU0652013A_4      96 non-null     float64       \n",
            " 20  Electricity_4      96 non-null     float64       \n",
            " 21  PNGASEUUSDM_5      96 non-null     float64       \n",
            " 22  PWHEAMTUSDM_5      96 non-null     float64       \n",
            " 23  WPU0652013A_5      96 non-null     float64       \n",
            " 24  Electricity_5      96 non-null     float64       \n",
            " 25  PNGASEUUSDM_6      96 non-null     float64       \n",
            " 26  PWHEAMTUSDM_6      96 non-null     float64       \n",
            " 27  WPU0652013A_6      96 non-null     float64       \n",
            " 28  Electricity_6      96 non-null     float64       \n",
            " 29  PNGASEUUSDM_7      96 non-null     float64       \n",
            " 30  PWHEAMTUSDM_7      96 non-null     float64       \n",
            " 31  WPU0652013A_7      96 non-null     float64       \n",
            " 32  Electricity_7      96 non-null     float64       \n",
            " 33  PNGASEUUSDM_8      96 non-null     float64       \n",
            " 34  PWHEAMTUSDM_8      96 non-null     float64       \n",
            " 35  WPU0652013A_8      96 non-null     float64       \n",
            " 36  Electricity_8      96 non-null     float64       \n",
            " 37  PNGASEUUSDM_9      96 non-null     float64       \n",
            " 38  PWHEAMTUSDM_9      96 non-null     float64       \n",
            " 39  WPU0652013A_9      96 non-null     float64       \n",
            " 40  Electricity_9      96 non-null     float64       \n",
            " 41  PNGASEUUSDM_10     96 non-null     float64       \n",
            " 42  PWHEAMTUSDM_10     96 non-null     float64       \n",
            " 43  WPU0652013A_10     96 non-null     float64       \n",
            " 44  Electricity_10     96 non-null     float64       \n",
            " 45  PNGASEUUSDM_11     96 non-null     float64       \n",
            " 46  PWHEAMTUSDM_11     96 non-null     float64       \n",
            " 47  WPU0652013A_11     96 non-null     float64       \n",
            " 48  Electricity_11     96 non-null     float64       \n",
            " 49  PNGASEUUSDM_12     96 non-null     float64       \n",
            " 50  PWHEAMTUSDM_12     96 non-null     float64       \n",
            " 51  WPU0652013A_12     96 non-null     float64       \n",
            " 52  Electricity_12     96 non-null     float64       \n",
            " 53  AR_1               96 non-null     float64       \n",
            " 54  AR_2               96 non-null     float64       \n",
            " 55  AR_3               96 non-null     float64       \n",
            " 56  AR_4               96 non-null     float64       \n",
            " 57  AR_5               96 non-null     float64       \n",
            " 58  AR_6               96 non-null     float64       \n",
            " 59  AR_7               96 non-null     float64       \n",
            " 60  AR_8               96 non-null     float64       \n",
            " 61  AR_9               96 non-null     float64       \n",
            " 62  AR_10              96 non-null     float64       \n",
            " 63  AR_11              96 non-null     float64       \n",
            " 64  AR_12              96 non-null     float64       \n",
            "dtypes: datetime64[ns](1), float64(61), int64(2), object(1)\n",
            "memory usage: 49.5+ KB\n",
            "None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-month prediction"
      ],
      "metadata": {
        "id": "hpbYMFy0Ky6_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end,):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year', 'Month', 'Average_price'],axis=1)\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.1, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "vwan5RQtF9o-"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)\n"
      ],
      "metadata": {
        "id": "-W84CREvMPCL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f442ae54-cca1-4e41-b1ec-87266d7e4b8a"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.068\n",
            "Alpha:  0.1\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.023\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.014\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.017\n",
            "Alpha:  0.1\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.145\n",
            "Alpha:  0.16332110703567856\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.059\n",
            "Alpha:  0.13721240413471159\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.034\n",
            "Alpha:  0.1\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.019\n",
            "Alpha:  0.1\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.063\n",
            "Alpha:  0.1\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.076\n",
            "Alpha:  0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "id": "GDKlZEd_MZO2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e8b96bbe-b4c0-4acd-9c3d-869152e1dec5"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.052\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-month prediction\n"
      ],
      "metadata": {
        "id": "XVVNycP3VlkM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.01, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "BWOJC-cqVlGi"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "Ivd5vIFmaG09",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4116719-dbd9-4023-febf-cb2e3b1bf32c"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.155\n",
            "Alpha:  0.16581193731243749\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.060\n",
            "Alpha:  0.1786862287429143\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.031\n",
            "Alpha:  0.14963654551517172\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.032\n",
            "Alpha:  0.173404468156052\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.282\n",
            "Alpha:  0.25461153717905965\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.206\n",
            "Alpha:  0.21202734244748248\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.138\n",
            "Alpha:  0.0796532177392464\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.035\n",
            "Alpha:  0.020563521173724574\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.064\n",
            "Alpha:  0.02452484161387129\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.129\n",
            "Alpha:  0.01990330110036679\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mItqOB7EaJaT",
        "outputId": "0ad9cb08-043e-4614-e4c9-35bf66053057"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.113\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-month prediction"
      ],
      "metadata": {
        "id": "UCybluNnb4C9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.0000001, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "bJSN9-mZb6BC"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "uo3_8_Z4cBUN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "813a7cfe-6b9f-4e70-aa71-128de5e007c5"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.013e-02, tolerance: 3.305e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.124\n",
            "Alpha:  0.30910310343447817\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.477e-02, tolerance: 3.823e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.090e-01, tolerance: 3.413e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.029e-02, tolerance: 3.677e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.930e-02, tolerance: 1.859e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.895e-03, tolerance: 3.228e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-07-01 to 2020-01-01: 0.105\n",
            "Alpha:  0.2017339911303768\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.900e-03, tolerance: 4.316e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.012e-01, tolerance: 3.846e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.482e-01, tolerance: 4.125e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.056e-01, tolerance: 1.316e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.439e-01, tolerance: 4.742e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2020-01-01 to 2020-07-01: 0.055\n",
            "Alpha:  0.2574192139713238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.165e-02, tolerance: 4.879e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.623e+00, tolerance: 4.879e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.863e-01, tolerance: 4.410e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.757e+00, tolerance: 2.864e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.300e-01, tolerance: 3.519e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.886e+00, tolerance: 5.174e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2020-07-01 to 2021-01-01: 0.054\n",
            "Alpha:  0.41380466015338446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.095e-02, tolerance: 5.509e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.127e+00, tolerance: 5.509e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 6.408e-01, tolerance: 5.134e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.311e-01, tolerance: 1.213e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.718e+00, tolerance: 5.626e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.836e+00, tolerance: 5.417e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2021-01-01 to 2021-07-01: 0.313\n",
            "Alpha:  0.41380466015338446\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.775e+00, tolerance: 6.025e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.343e+00, tolerance: 6.084e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 3.276e+00, tolerance: 5.056e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.109e+00, tolerance: 6.437e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.191e+00, tolerance: 2.249e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2021-07-01 to 2022-01-01: 0.415\n",
            "Alpha:  0.3261087702900967\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.210e+00, tolerance: 6.481e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.507e+00, tolerance: 6.704e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.497e+00, tolerance: 6.881e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.107e+00, tolerance: 6.651e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.094e+00, tolerance: 1.064e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2022-01-01 to 2022-07-01: 0.123\n",
            "Alpha:  0.14104710156718905\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.850e+00, tolerance: 6.900e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.802e+00, tolerance: 7.248e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.543e+00, tolerance: 7.554e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 8.746e-01, tolerance: 7.095e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.271e-01, tolerance: 6.732e-04\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2022-07-01 to 2023-01-01: 0.145\n",
            "Alpha:  0.01500510016672224\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 7.937e-03, tolerance: 7.904e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.622e+00, tolerance: 7.263e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.569e+00, tolerance: 7.803e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.165e+00, tolerance: 7.904e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 9.132e-01, tolerance: 6.985e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 4.226e-01, tolerance: 2.084e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2023-01-01 to 2023-07-01: 0.215\n",
            "Alpha:  0.01700576685561854\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.198e-02, tolerance: 8.371e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.385e+00, tolerance: 7.590e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 2.233e+00, tolerance: 8.371e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.692e+00, tolerance: 8.146e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.328e+00, tolerance: 6.853e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n",
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.544e-01, tolerance: 3.506e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2023-07-01 to 2024-01-01: 0.374\n",
            "Alpha:  0.03101043367789263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPztBxN1cBzA",
        "outputId": "fb32c761-ae48-4432-a134-326751fd8341"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.192\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0004"
      ],
      "metadata": {
        "id": "8GpF37D5S_qa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = df_0004"
      ],
      "metadata": {
        "id": "1pj-h-BYS_qn"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [column for column in feature_df.columns if 'RM' in column]\n",
        "feature_df = feature_df.drop(columns=columns_to_drop)\n",
        "print(feature_df.info())"
      ],
      "metadata": {
        "id": "E2KFz56PS_qo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-month prediction"
      ],
      "metadata": {
        "id": "JrhGU1Y8S_qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end,):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year', 'Month', 'Average_price'],axis=1)\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.1, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "6Oszir5uS_qo"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "J1GAcF9WS_qo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a5627468-f59c-4a8f-ec0d-70e57b09cc8f"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.071\n",
            "Alpha:  0.1\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.120\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.123\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.189\n",
            "Alpha:  0.1\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.116\n",
            "Alpha:  0.1\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.086\n",
            "Alpha:  0.1\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.103\n",
            "Alpha:  0.1\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.085\n",
            "Alpha:  0.1\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.073\n",
            "Alpha:  0.1\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.032\n",
            "Alpha:  0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41a1991a-b792-4b84-8284-b91e1c94355a",
        "id": "0AiTWDsaS_qo"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-month prediction\n"
      ],
      "metadata": {
        "id": "jSC1qqLmS_qo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.2, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "3fllH5FjS_qo"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "PHhp5L9PS_qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d0fa9da-c8fe-4be7-fae1-86e7b92b3575"
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.113\n",
            "Alpha:  0.2\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.176\n",
            "Alpha:  0.2\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.232\n",
            "Alpha:  0.2\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.177\n",
            "Alpha:  0.2\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.170\n",
            "Alpha:  0.2\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.233\n",
            "Alpha:  0.2\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.103\n",
            "Alpha:  0.2\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.185\n",
            "Alpha:  0.2\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.155\n",
            "Alpha:  0.2\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.036\n",
            "Alpha:  0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2984e53-aa9d-48f3-f2e2-fcb4bfe994ab",
        "id": "NBndnJZaS_qp"
      },
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.158\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-month prediction"
      ],
      "metadata": {
        "id": "SWIk7KBbS_qp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.3, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "fE7DRyFCS_qp"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "_Q9VVfWhS_qp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "85466eb8-9e31-483f-b87a-ef916196f622"
      },
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.097\n",
            "Alpha:  0.3\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.262\n",
            "Alpha:  0.3\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.299\n",
            "Alpha:  0.3\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.352\n",
            "Alpha:  0.3\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.199\n",
            "Alpha:  0.3\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.372\n",
            "Alpha:  0.3\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.267\n",
            "Alpha:  0.32894298099366454\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.095\n",
            "Alpha:  0.3\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.495\n",
            "Alpha:  0.3\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.045\n",
            "Alpha:  0.43234411470490164\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff25fc0d-f296-4088-e598-594b67b484d1",
        "id": "FKlM71GjS_qp"
      },
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.248\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0006"
      ],
      "metadata": {
        "id": "xqMqNRYgTIER"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = df_0006"
      ],
      "metadata": {
        "id": "sHs5v5G6TIEd"
      },
      "execution_count": 137,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [column for column in feature_df.columns if 'RM' in column]\n",
        "feature_df = feature_df.drop(columns=columns_to_drop)\n",
        "print(feature_df.info())"
      ],
      "metadata": {
        "id": "n4kgzBLhTIEd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-month prediction"
      ],
      "metadata": {
        "id": "_YkFXPWQTIEd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end,):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year', 'Month', 'Average_price'],axis=1)\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.1, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "NEocBAyeTIEd"
      },
      "execution_count": 176,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "H5OL9LCaTIEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06a7623f-a48f-4dd6-b52d-65c31e48bed4"
      },
      "execution_count": 177,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.053\n",
            "Alpha:  0.152217405801934\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.052\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.075\n",
            "Alpha:  0.1\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.047\n",
            "Alpha:  0.181027009003001\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.065\n",
            "Alpha:  0.1\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.150\n",
            "Alpha:  0.15881960653551186\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.277\n",
            "Alpha:  0.1\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.255\n",
            "Alpha:  0.4352117372457486\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.067\n",
            "Alpha:  0.1\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.119\n",
            "Alpha:  0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74fea93f-8f04-4e76-b220-78fdfbb57ad5",
        "id": "dWlbJJZxTIEe"
      },
      "execution_count": 178,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-month prediction\n"
      ],
      "metadata": {
        "id": "VkIzgNejTIEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.2, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "_aUGkhYMTIEe"
      },
      "execution_count": 188,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "QvNw6ublTIEe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2270a589-779a-47f6-bb40-b0dc7956d580"
      },
      "execution_count": 189,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.053\n",
            "Alpha:  0.2\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.048\n",
            "Alpha:  0.2\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.064\n",
            "Alpha:  0.2\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.042\n",
            "Alpha:  0.2466822274091364\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.055\n",
            "Alpha:  0.2\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.163\n",
            "Alpha:  0.2\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.540\n",
            "Alpha:  0.4662220740246749\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.236\n",
            "Alpha:  0.41180393464488163\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.063\n",
            "Alpha:  0.2\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.120\n",
            "Alpha:  0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "209d65b7-152f-4564-c3a8-0e060c722ae4",
        "id": "FmKFQTS0TIEe"
      },
      "execution_count": 190,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.138\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-month prediction"
      ],
      "metadata": {
        "id": "jfCFLOLrTIEe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.6, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "B57QN8iPTIEe"
      },
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "B8-H7Q2hTIEf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1f95d93b-fda2-45d2-e3c3-b2107a0a62be"
      },
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.052\n",
            "Alpha:  0.6\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.037\n",
            "Alpha:  0.6\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.049\n",
            "Alpha:  0.6\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.048\n",
            "Alpha:  0.8402134044681561\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.062\n",
            "Alpha:  0.6\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.241\n",
            "Alpha:  0.6\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.601\n",
            "Alpha:  0.6\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.442\n",
            "Alpha:  0.6018672890963654\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.098\n",
            "Alpha:  0.6\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.091\n",
            "Alpha:  0.6\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43ef4c28-60e7-45bb-a8c9-8deb8111c275",
        "id": "XsH2GDThTIEf"
      },
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.172\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 0007"
      ],
      "metadata": {
        "id": "LVHrPQN8TNEl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "feature_df = df_0007"
      ],
      "metadata": {
        "id": "1IVCg69fTNEw"
      },
      "execution_count": 233,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = [column for column in feature_df.columns if 'RM' in column]\n",
        "feature_df = feature_df.drop(columns=columns_to_drop)\n",
        "print(feature_df.info())"
      ],
      "metadata": {
        "id": "PvrLWxEmTNEx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 1-month prediction"
      ],
      "metadata": {
        "id": "QFL8waXXTNEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end,):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year', 'Month', 'Average_price'],axis=1)\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.01, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "9ZW1a4seTNEx"
      },
      "execution_count": 241,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "4DWi51FlTNEx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72f51f06-aa41-41ee-bf79-6b88e05f64f1"
      },
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.470e-03, tolerance: 1.455e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.404\n",
            "Alpha:  0.9894364788262754\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.139\n",
            "Alpha:  0.19321107035678559\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.263\n",
            "Alpha:  0.014621540513504502\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.091\n",
            "Alpha:  0.06017672557519173\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.175\n",
            "Alpha:  0.05753584528176058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/linear_model/_coordinate_descent.py:631: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations, check the scale of the features or consider increasing regularisation. Duality gap: 1.303e-02, tolerance: 3.171e-03\n",
            "  model = cd_fast.enet_coordinate_descent(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2021-07-01 to 2022-01-01: 0.218\n",
            "Alpha:  0.04367122374124708\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.511\n",
            "Alpha:  0.9263854618206068\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.477\n",
            "Alpha:  0.7329409803267756\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.051\n",
            "Alpha:  0.05093364454818273\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.061\n",
            "Alpha:  0.03640880293431144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fde42297-824c-4bb0-c0a9-4696e0c52df9",
        "id": "ML2dIfVRTNEx"
      },
      "execution_count": 243,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.239\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3-month prediction\n"
      ],
      "metadata": {
        "id": "I85N0a6VTNEx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.1, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "RXIVfcQtTNEx"
      },
      "execution_count": 246,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "sv9isBcNTNEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8fec256c-992e-4f89-e43d-be9afd8e50a4"
      },
      "execution_count": 247,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.404\n",
            "Alpha:  0.8517505835278426\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.135\n",
            "Alpha:  0.11890630210070025\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.320\n",
            "Alpha:  0.12400800266755585\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.123\n",
            "Alpha:  0.14321440480160053\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.205\n",
            "Alpha:  0.1\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.266\n",
            "Alpha:  0.6404801600533511\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.439\n",
            "Alpha:  0.5474491497165722\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.447\n",
            "Alpha:  0.6764921640546849\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.179\n",
            "Alpha:  0.33617872624208067\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.124\n",
            "Alpha:  0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a1ec44ac-717e-4e11-d0d7-a3ab0bbf9920",
        "id": "FQVzmMLrTNEy"
      },
      "execution_count": 248,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.264\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6-month prediction"
      ],
      "metadata": {
        "id": "dI7M7mRQTNEy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_mape(test_start, test_end):\n",
        "  # Split data into train and test sets\n",
        "  train_df = feature_df[(feature_df['Time'] > '2015-12-31') & (feature_df['Time'] < test_start)]\n",
        "  test_df = feature_df[(feature_df['Time'] >= test_start) & (feature_df['Time'] < test_end)]\n",
        "\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "  # Create X, y\n",
        "  X_train = train_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_train = X_train[X_train.columns.drop(list(X_train.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "  X_test = test_df.drop(['Time', 'Group Description', 'Year','Month','Average_price'],axis=1)\n",
        "  X_test = X_test[X_test.columns.drop(list(X_test.filter(regex='_1$|_2$|_3$|_4$|_5$')))]\n",
        "\n",
        "  y_train = train_df['Average_price'].values\n",
        "  y_test = test_df['Average_price'].values\n",
        "  ## ------ This part needs to be changed for each forecasting horizon\n",
        "\n",
        "  # Standardlisation\n",
        "  scaler_x = StandardScaler()\n",
        "  X_train_scaled = scaler_x.fit_transform(X_train)\n",
        "  X_test_scaled = scaler_x.transform(X_test)\n",
        "  scaler_y = StandardScaler()\n",
        "  y_train_scaled = scaler_y.fit_transform(y_train.reshape(-1,1))\n",
        "  y_test_scaled = scaler_y.transform(y_test.reshape(-1,1))\n",
        "  # Define the parameter grid\n",
        "  param_grid = {'alpha': np.linspace(0.2, 1, 3000)}\n",
        "  # Create a Lasso regression model\n",
        "  lasso = Lasso()\n",
        "  # Create RandomizedSearchCV object\n",
        "  random_search = RandomizedSearchCV(estimator=lasso,\n",
        "                                   param_distributions=param_grid,\n",
        "                                   n_iter=300,\n",
        "                                   cv=5,\n",
        "                                   random_state=42)\n",
        "  # Fit the data to perform a grid search\n",
        "  random_search.fit(X_train_scaled, y_train_scaled)\n",
        "  assert random_search.n_features_in_ == len(X_train.columns)\n",
        "  # Get the best Lasso model from RandomizedSearchCV\n",
        "  best_lasso_model = random_search.best_estimator_\n",
        "  # Predict on the test data\n",
        "  y_pred_test = best_lasso_model.predict(X_test_scaled)\n",
        "  y_pred_test_inverse = scaler_y.inverse_transform(y_pred_test.reshape(-1,1)) # the model was trained with log-transformed and standardlised y\n",
        "  mape = round(mean_absolute_percentage_error(y_test,y_pred_test_inverse), 3)\n",
        "  alpha = random_search.best_params_['alpha']\n",
        "  return (mape, alpha)"
      ],
      "metadata": {
        "id": "PuOh7nkqTNEy"
      },
      "execution_count": 271,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mape_values = []\n",
        "for start, end in test_periods:\n",
        "    mape, alpha = calculate_mape(start, end)\n",
        "    mape_values.append(mape)\n",
        "    print(f\"MAPE from {start} to {end}: {mape:.3f}\")\n",
        "    print('Alpha: ', alpha)"
      ],
      "metadata": {
        "id": "syzjt5DKTNEy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f9c2f4-7a1e-4a8b-86ba-7847a8e9df93"
      },
      "execution_count": 272,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAPE from 2019-01-01 to 2019-07-01: 0.404\n",
            "Alpha:  0.8682227409136378\n",
            "MAPE from 2019-07-01 to 2020-01-01: 0.174\n",
            "Alpha:  0.32004001333777926\n",
            "MAPE from 2020-01-01 to 2020-07-01: 0.320\n",
            "Alpha:  0.2\n",
            "MAPE from 2020-07-01 to 2021-01-01: 0.114\n",
            "Alpha:  0.2\n",
            "MAPE from 2021-01-01 to 2021-07-01: 0.172\n",
            "Alpha:  0.2405468489496499\n",
            "MAPE from 2021-07-01 to 2022-01-01: 0.266\n",
            "Alpha:  0.6804268089363121\n",
            "MAPE from 2022-01-01 to 2022-07-01: 0.511\n",
            "Alpha:  0.6804268089363121\n",
            "MAPE from 2022-07-01 to 2023-01-01: 0.573\n",
            "Alpha:  0.9978659553184395\n",
            "MAPE from 2023-01-01 to 2023-07-01: 0.166\n",
            "Alpha:  0.4739579859953318\n",
            "MAPE from 2023-07-01 to 2024-01-01: 0.058\n",
            "Alpha:  0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "average_mape = np.mean(mape_values)\n",
        "print(f\"Average MAPE: {average_mape:.3f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9e275a98-94d3-44ef-f918-0259f3139117",
        "id": "QCb4juvSTNEy"
      },
      "execution_count": 273,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MAPE: 0.276\n"
          ]
        }
      ]
    }
  ]
}